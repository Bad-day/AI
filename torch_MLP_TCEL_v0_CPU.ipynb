{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pUwNg7XGxOfP"
      },
      "outputs": [],
      "source": [
        "# MLP by pyTorch, CPU version\n",
        "# Low level implementation (No Model)\n",
        "# 4 class : T/C/E/L\n",
        "# 3 patterns / 1 class\n",
        "# 2 Layer\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "\n",
        "import torch\n",
        "\n",
        "#from torch import tensor\n",
        "#from torchvision import datasets\n",
        "#from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "lrate = 0.01\n",
        "INDIM   = 26\n",
        "H1DIM    = 10\n",
        "H2DIM    = 10\n",
        "OUTDIM  = 4\n",
        "\n",
        "PTTN_NUM    = 12\n",
        "\n",
        "#x  = np.array([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "x  = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-1\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,\n",
        "                     0.0, 0.0, 1.0, 0.0, 1.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-2\n",
        "               [1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-3\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-1\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-2\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #C-3\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-1\n",
        "               [1.0, 0.5, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-2\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     0.5, 1.0, 1.0, 1.0, 1.0 ],  #E-3               \n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #L-1\n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #L-2\n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     0.3, 1.0, 1.0, 1.0, 1.0 ] ] )  #L-3                            \n",
        "                              \n",
        "#t  = np.array([ [1.0, 0.0, 0.0, 0.0],\n",
        "yt  = torch.tensor([ [1.0, 0.0, 0.0, 0.0],               \n",
        "                [1.0, 0.0, 0.0, 0.0],\n",
        "                [1.0, 0.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 0.0, 1.0],\n",
        "                [0.0, 0.0, 0.0, 1.0],\n",
        "                [0.0, 0.0, 0.0, 1.0] ])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(yt.shape)"
      ],
      "metadata": {
        "id": "DuYoQs2v0rxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1 Layer MLP ==> SLP, no sigmoid activation\n",
        "### variables & gradient setting\n",
        "w = torch.randn(INDIM, OUTDIM, requires_grad=True)\n",
        "b = torch.randn(OUTDIM, requires_grad=True)\n",
        "\n",
        "### Loss & Optimizer\n",
        "#loss = torch.nn.functional.binary_cross_entropy_with_logits(z, yt)\n",
        "floss = nn.MSELoss()\n",
        "optimizer = optim.SGD([w, b], lr=lrate)\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "###############################\n",
        "### Test 1 epoch\n",
        "'''\n",
        "optimizer.zero_grad()\n",
        "z = torch.matmul(x, w)+b\n",
        "#print(x.shape, w.shape, z.shape)\n",
        "loss = floss(z, yt)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "'''\n",
        "###############################\n",
        "for epoch in range(20000) :\n",
        "  optimizer.zero_grad()\n",
        "  zt = torch.matmul(x, w)+b\n",
        "  z = sigmoid(zt)\n",
        "\n",
        "  loss = floss(z, yt)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch%1000 == 0 :\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"z:\", z)\n"
      ],
      "metadata": {
        "id": "PFWM774cyZUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1 Layer MLP ==> SLP\n",
        "### variables & gradient setting\n",
        "W1 = torch.randn(INDIM, OUTDIM, requires_grad=True)\n",
        "B1 = torch.randn(OUTDIM, requires_grad=True)\n",
        "\n",
        "### Loss & Optimizer\n",
        "#loss = torch.nn.functional.binary_cross_entropy_with_logits(z, yt)\n",
        "floss = nn.MSELoss()\n",
        "optimizer = optim.SGD([W1, B1], lr=lrate)\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "###############################\n",
        "for epoch in range(20000) :\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  zt = torch.matmul(x, W1)+B1\n",
        "  z = sigmoid(zt)\n",
        "\n",
        "  loss = floss(z, yt)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%1000 == 0 :\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"z:\", z)\n"
      ],
      "metadata": {
        "id": "SIQrch_eAmYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2 Layer MLP\n",
        "### variables & gradient setting\n",
        "W1 = torch.randn(INDIM, H1DIM, requires_grad=True)\n",
        "B1 = torch.randn(H1DIM, requires_grad=True)\n",
        "W2 = torch.randn(H1DIM, OUTDIM, requires_grad=True)\n",
        "B2 = torch.randn(OUTDIM, requires_grad=True)\n",
        "\n",
        "### Loss & Optimizer\n",
        "#loss = torch.nn.functional.binary_cross_entropy_with_logits(z, yt)\n",
        "floss = nn.MSELoss()\n",
        "optimizer = optim.SGD([W1, B1, W2, B2], lr=lrate)\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "###############################\n",
        "for epoch in range(20000) :\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  z1t = torch.matmul(x, W1)+B1\n",
        "  z1 = sigmoid(z1t)\n",
        "  z2t = torch.matmul(z1, W2)+B2\n",
        "  z2 = sigmoid(z2t)\n",
        "\n",
        "  loss = floss(z2, yt)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%1000 == 0 :\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"z:\", z2)\n"
      ],
      "metadata": {
        "id": "NESllsdq52Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3 Layer MLP\n",
        "### variables & gradient setting\n",
        "W1 = torch.randn(INDIM, H1DIM, requires_grad=True)\n",
        "B1 = torch.randn(H1DIM, requires_grad=True)\n",
        "W2 = torch.randn(H1DIM, H2DIM, requires_grad=True)\n",
        "B2 = torch.randn(H2DIM, requires_grad=True)\n",
        "W3 = torch.randn(H2DIM, OUTDIM, requires_grad=True)\n",
        "B3 = torch.randn(OUTDIM, requires_grad=True)\n",
        "\n",
        "### Loss & Optimizer\n",
        "#loss = torch.nn.functional.binary_cross_entropy_with_logits(z, yt)\n",
        "floss = nn.MSELoss()\n",
        "optimizer = optim.SGD([W1, B1, W2, B2, W3, B3], lr=lrate)\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "###############################\n",
        "for epoch in range(20000) :\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  z1t = torch.matmul(x, W1)+B1\n",
        "  z1 = sigmoid(z1t)\n",
        "  z2t = torch.matmul(z1, W2)+B2\n",
        "  z2 = sigmoid(z2t)\n",
        "  z3t = torch.matmul(z2, W3)+B3\n",
        "  z3 = sigmoid(z3t)\n",
        "\n",
        "  loss = floss(z3, yt)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%1000 == 0 :\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"z:\", z3)\n"
      ],
      "metadata": {
        "id": "MopUI0XW9Orm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}