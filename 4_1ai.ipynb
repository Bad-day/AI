{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV9WrcDvd7k0yf4H0IN7cw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bad-day/AI/blob/master/4_1ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgMD2hxt0CuY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "dec60e8f-9224-4894-bb26-9d68a9fca2bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e32df9ee1cb5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sepal.width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sepal.width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/iris.csv'"
          ]
        }
      ],
      "source": [
        "#과제1 범위지정, 꽃잎의 두께 평균,최대,최소 구하기기 \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "data = pd.read_csv('/content/iris.csv')\n",
        "max= data.loc[51:100, 'sepal.width'].max()\n",
        "min= data.loc[51:100, 'sepal.width'].mean()\n",
        "avr= data.loc[51:100, 'sepal.width'].min()\n",
        "print('꽃잎의 두께 평균:', avr)\n",
        "print('꽃잎의 두께 최대값:', max)\n",
        "print('꽃잎의 두께 최소값:', min)\n",
        "# iris_petal_width_max, iris_petal_width_mean, iris_petal_width_min = np.max(train_df['petal.length']), np.mean(train_df['petal.length']), np.min(train_df['petal.length'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "k = np.random.randint(1,21, size=1)\n",
        "print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQf5TusuQEAt",
        "outputId": "349da574-90fe-4de9-ef80-6e28fcdbe6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# iris.csv 파일 불러오기\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# petal length와 petal width만 사용\n",
        "#, 2,3 -> petal len , petal wid\n",
        "# sepal len wid , petal len wid\n",
        "#                  2         3\n",
        "data = df.iloc[:, [0, 1]]\n",
        "# 클래스별 데이터 나누기\n",
        "setosa = data[df['variety'] == 'Setosa']\n",
        "versicolor = data[df['variety'] == 'Versicolor']\n",
        "virginica = data[df['variety'] == 'Virginica']\n",
        "\n",
        "# 평균과 분산 구하기\n",
        "setosa_mean = setosa.mean()\n",
        "versicolor_mean = versicolor.mean()\n",
        "virginica_mean = virginica.mean()\n",
        "\n",
        "setosa_var = setosa.var()\n",
        "versicolor_var = versicolor.var()\n",
        "virginica_var = virginica.var()\n",
        "result = (setosa_var + versicolor_var + virginica_var)/3\n",
        "print(\"분산 \",result)\n",
        "# print('setosa 평균:', setosa_mean)\n",
        "# print('versicolor 평균:', versicolor_mean)\n",
        "# print('virginica 평균:', virginica_mean)\n",
        "print('setosa 분산:', setosa_var)\n",
        "print('versicolor 분산:', versicolor_var)\n",
        "print('virginica 분산:', virginica_var)\n"
      ],
      "metadata": {
        "id": "S75nXtdGT0pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "x = np.array([(123,167,72,87,98])\n",
        "y = 78+87+57+75+100\n",
        "\n",
        "x_mean = x/5\n",
        "y_mean = y/5\n",
        "print(x_mean,y_mean)\n",
        "\n",
        "a = np.sum()"
      ],
      "metadata": {
        "id": "wYPYFVIPBSSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4701c8a7-fcdc-418c-b43f-0160d65a5d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109.4 79.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dt = pd.read_csv('/content/drive/MyDrive/iris.csv')\n",
        "# iris.csv 파일을 읽어와서 리스트에 저장합니다.\n",
        "p = [row for row in dt]\n",
        "\n",
        "# 입력 데이터를 실수형으로 변환합니다.\n",
        "k = list(map(float, input(\"float 형태 수 4개를 입력하세요: \").split()))\n",
        "\n",
        "# 가장 근사한 항목을 계산합니다.\n",
        "distances = []\n",
        "for row in p[1:]:\n",
        "    distance = sum([(float(row[i]) - k[i])**2 for i in range(4)])\n",
        "    distances.append(distance)\n",
        "index = distances.index(min(distances)) + 2\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "print(f\"{index}번째 열, variety: ({p[index][4]}).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "7Fu1jB-kpIOJ",
        "outputId": "afaac2c0-e49f-46a5-aa5c-a989078f0c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "float 형태 수 4개를 입력하세요: 2 3 4 5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-15ba2dfcff49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-15ba2dfcff49>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 's'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 과제2 float4개 입력받고, 가장 가까운 행 출력력\n",
        "\n",
        "# 데이터 불러오기\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 입력값 받기\n",
        "input_sepal_l, input_sepal_w, input_petal_l, input_petal_w = map(float, input().split())\n",
        "\n",
        "# 데이터프레임 불러오기\n",
        "df = pd.read_csv(\"/content/iris.csv\")\n",
        "\n",
        "# 각 속성값 추출\n",
        "print_attri = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "\n",
        "# 유클리드 거리 계산\n",
        "res = {}\n",
        "for i, data in enumerate(print_attri):\n",
        "    distance = (input_sepal_l - data[0]) ** 2 + (input_sepal_w - data[1]) ** 2 + \\\n",
        "               (input_petal_l - data[2]) ** 2 + (input_petal_w - data[3]) ** 2\n",
        "    res[i] = distance\n",
        "\n",
        "# 결과 출력\n",
        "for i in range(5):\n",
        "    index = min(res, key=res.get)\n",
        "    print(f'가장 가까운 값은 {index}행 값 입니다.:')\n",
        "    print(f'{df.loc[index]}')\n",
        "    print(f'오차:{res[index]}')\n",
        "    print('---------------------------')\n",
        "    del res[index]   # 해당 인덱스의 결과를 삭제\n",
        "\n",
        "\n",
        "# 주석 추가\n",
        "\"\"\"\n",
        "- 각 속성값을 추출해 print_attri에 저장\n",
        "- 각 속성값과 입력값 사이의 거리(distance) 계산 후 res 딕셔너리에 저장\n",
        "- 가장 거리가 짧은 인덱스를 찾아 해당하는 행 출력 후 res 딕셔너리에서 해당 결과 삭제\n",
        "- 5번 반복하여 가장 가까운 5개의 결과 출력\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Wi13sIhQadLr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "043b790d-1e95-4b95-ee30-bb2aec43311b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-564d46046569>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 입력값 받기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minput_sepal_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sepal_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_petal_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_petal_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 데이터프레임 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#과제4 50개씩 a, b, c로 나눠 각각 요소의 평균을 구한뒤 행마다 유클리드 거리 결과 출력\n",
        "import numpy as np;import pandas as pd\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "attri_values = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
        "meana,meanb,meanc = df.iloc[0:51, :], df.iloc[52:101, :], df.iloc[102:151, :]\n",
        "m = random.sample(list(attri_values),3)\n",
        "m1=[]\n",
        "m2=[]\n",
        "m3=[]\n",
        "a =[]\n",
        "def calc_distance(row, means):\n",
        "  sepal_l, sepal_w, petal_l, petal_w = row\n",
        "  mean_sepal_l, mean_sepal_w, mean_petal_l, mean_petal_w = means\n",
        "  distance = (sepal_l - mean_sepal_l) ** 2 + (sepal_w - mean_sepal_w) ** 2 + (petal_l - mean_petal_l) ** 2 + (petal_w - mean_petal_w) ** 2\n",
        "  return distance\n",
        "\n",
        "for i in range(10):\n",
        "  for j in attri_values:\n",
        "    j=list(j)\n",
        "    a=[]\n",
        "    distance1 = calc_distance(j,m[0])\n",
        "    distance2 = calc_distance(j,m[1])\n",
        "    distance3 = calc_distance(j,m[2])\n",
        "    a.append(distance1);a.append(distance2);a.append(distance3)\n",
        "    print(a)\n",
        "    if a.index(min(a)) == 0:\n",
        "      m1.append(j)\n",
        "    elif a.index(min(a)) ==1:\n",
        "      m2.append(j)\n",
        "    elif a.index(min(a)) ==2:\n",
        "      m3.append(j)\n",
        "  for i in m1:\n",
        "  m1=sum(m1)/len(m1)\n",
        "  m2=sum(m2)/len(m2)\n",
        "  m3=sum(m3)/len(m3)\n",
        "  m=[]\n",
        "  m.append(m1)\n",
        "  m.append(m2)\n",
        "  m.append(m3)\n",
        "\n",
        "print(m)\n",
        "\n",
        "# for i, row in enumerate(attri_values):\n",
        "#   if i < 50:\n",
        "#     distance = calc_distance(row, means_a)\n",
        "#     print(f\"{i+1}행 distance_a : {distance:.2f}\")\n",
        "#   elif i < 100:\n",
        "#     distance = calc_distance(row, means_b)\n",
        "#     print(f\"{i+1}행 distance_b : {distance:.2f}\")\n",
        "#   else:\n",
        "#     distance = calc_distance(row, means_c)\n",
        "#     print(f\"{i+1}행 distance_c : {distance:.2f}\")"
      ],
      "metadata": {
        "id": "50eyZbBE49dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "75120df3-5268-4ecf-fee8-e0ffa92cbb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.700000000000001, 6.660000000000001, 9.900000000000002]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-9afe8911b106>:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
            "<ipython-input-23-9afe8911b106>:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
            "<ipython-input-23-9afe8911b106>:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9afe8911b106>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# 꽃의 종류별로 학습 데이터와 검증 데이터를 나눈다.\n",
        "leranData = pd.concat([df[df['variety'] == 'Setosa'].iloc[:40], \n",
        "                      df[df['variety'] == 'Versicolor'].iloc[:40],\n",
        "                      df[df['variety'] == 'Virginica'].iloc[:40]])  #train\n",
        "evalData = pd.concat([df[df['variety'] == 'Setosa'].iloc[40:], \n",
        "                     df[df['variety'] == 'Versicolor'].iloc[40:],\n",
        "                     df[df['variety'] == 'Virginica'].iloc[40:]]) #test\n",
        "# 1) 클래스별로 40:10으로 데이터를 leranData의[120][2], evalData[30][2]나누어 읽기기\n",
        "print(leranData,\"현재 leranData의 개수 : \",len(leranData))\n",
        "print(evalData, \"현재 evalData의 개수 : \",len(evalData))\n",
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def normalize(df):\n",
        "  result = df.copy()\n",
        "  for feature_name in df.columns:\n",
        "    if feature_name != 'class':\n",
        "      max_value = df[feature_name].max()\n",
        "      min_value = df[feature_name].min()\n",
        "      result[feature_name] = sigmoid((df[feature_name] - min_value) / (max_value - min_value))\n",
        "  return result\n",
        "\n",
        "# 학습 데이터에서 petal.width와 petal.length 컬럼의 값을 가져온다.\n",
        "X_train = leranData[['petal.width', 'petal.length']].values\n",
        "\n",
        "# K-Means 알고리즘을 구현한다.\n",
        "K = 3  # 군집 개수\n",
        "max_iter = 100  # 최대 반복 횟수\n",
        "\n",
        "# 초기 중심점을 랜덤하게 선택한다.\n",
        "centroids = X_train[np.random.choice(X_train.shape[0], K, replace=False)]\n",
        "\n",
        "# K-Means 알고리즘을 실행한다.\n",
        "for i in range(max_iter):\n",
        "    # 군집 할당 단계\n",
        "    distances = np.sqrt(((X_train - centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "    labels = np.argmin(distances, axis=0)\n",
        "    \n",
        "    # 중심점 업데이트 단계\n",
        "    new_centroids = np.array([X_train[labels == k].mean(axis=0) for k in range(K)])\n",
        "    \n",
        "    # 중심점이 더 이상 변하지 않으면 반복을 중지한다.\n",
        "    if np.allclose(centroids, new_centroids):\n",
        "        break\n",
        "    \n",
        "    centroids = new_centroids\n",
        "\n",
        "# 검증 데이터에서 군집 할당 결과를 확인한다.\n",
        "X_test = evalData[['petal.width', 'petal.length']].values\n",
        "test_distances = np.sqrt(((X_test - centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "test_labels = np.argmin(test_distances, axis=0)\n",
        "\n",
        "# 검증 결과를 출력한다.\n",
        "evalData['predicted'] = test_labels\n",
        "print(evalData[['variety', 'predicted']])\n",
        "\n",
        "train_indices = np.concatenate([np.arange(i*40, (i+1)*40-10) for i in range(3)])\n",
        "test_indices = np.concatenate([np.arange(i*40+30, (i+1)*40) for i in range(3)])\n",
        "train_df = df.iloc[train_indices, [2, 3]]\n",
        "test_df = df.iloc[test_indices, [2, 3]]\n",
        "\n",
        "# 각 꽃의 평균과 분산 계산\n",
        "for flower_name in ['Setosa', 'Versicolor', 'Virginica']:\n",
        "    flower_data = train_df[df['variety'] == flower_name]\n",
        "    flower_mean = flower_data.mean()\n",
        "    flower_var = flower_data.var()\n",
        "    print(f'Flower: {flower_name}')\n",
        "    print(f'Mean: {flower_mean.values}')\n",
        "    print(f'Variance: {flower_var.values}')"
      ],
      "metadata": {
        "id": "D79qARNayTUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e38c909-6b47-4ac6-fbb8-4001e80f104a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
            "0             5.1          3.5           1.4          0.2     Setosa\n",
            "1             4.9          3.0           1.4          0.2     Setosa\n",
            "2             4.7          3.2           1.3          0.2     Setosa\n",
            "3             4.6          3.1           1.5          0.2     Setosa\n",
            "4             5.0          3.6           1.4          0.2     Setosa\n",
            "..            ...          ...           ...          ...        ...\n",
            "135           7.7          3.0           6.1          2.3  Virginica\n",
            "136           6.3          3.4           5.6          2.4  Virginica\n",
            "137           6.4          3.1           5.5          1.8  Virginica\n",
            "138           6.0          3.0           4.8          1.8  Virginica\n",
            "139           6.9          3.1           5.4          2.1  Virginica\n",
            "\n",
            "[120 rows x 5 columns] 현재 leranData의 개수 :  120\n",
            "     sepal.length  sepal.width  petal.length  petal.width     variety\n",
            "40            5.0          3.5           1.3          0.3      Setosa\n",
            "41            4.5          2.3           1.3          0.3      Setosa\n",
            "42            4.4          3.2           1.3          0.2      Setosa\n",
            "43            5.0          3.5           1.6          0.6      Setosa\n",
            "44            5.1          3.8           1.9          0.4      Setosa\n",
            "45            4.8          3.0           1.4          0.3      Setosa\n",
            "46            5.1          3.8           1.6          0.2      Setosa\n",
            "47            4.6          3.2           1.4          0.2      Setosa\n",
            "48            5.3          3.7           1.5          0.2      Setosa\n",
            "49            5.0          3.3           1.4          0.2      Setosa\n",
            "90            5.5          2.6           4.4          1.2  Versicolor\n",
            "91            6.1          3.0           4.6          1.4  Versicolor\n",
            "92            5.8          2.6           4.0          1.2  Versicolor\n",
            "93            5.0          2.3           3.3          1.0  Versicolor\n",
            "94            5.6          2.7           4.2          1.3  Versicolor\n",
            "95            5.7          3.0           4.2          1.2  Versicolor\n",
            "96            5.7          2.9           4.2          1.3  Versicolor\n",
            "97            6.2          2.9           4.3          1.3  Versicolor\n",
            "98            5.1          2.5           3.0          1.1  Versicolor\n",
            "99            5.7          2.8           4.1          1.3  Versicolor\n",
            "140           6.7          3.1           5.6          2.4   Virginica\n",
            "141           6.9          3.1           5.1          2.3   Virginica\n",
            "142           5.8          2.7           5.1          1.9   Virginica\n",
            "143           6.8          3.2           5.9          2.3   Virginica\n",
            "144           6.7          3.3           5.7          2.5   Virginica\n",
            "145           6.7          3.0           5.2          2.3   Virginica\n",
            "146           6.3          2.5           5.0          1.9   Virginica\n",
            "147           6.5          3.0           5.2          2.0   Virginica\n",
            "148           6.2          3.4           5.4          2.3   Virginica\n",
            "149           5.9          3.0           5.1          1.8   Virginica 현재 evalData의 개수 :  30\n",
            "        variety  predicted\n",
            "40       Setosa          0\n",
            "41       Setosa          0\n",
            "42       Setosa          0\n",
            "43       Setosa          0\n",
            "44       Setosa          0\n",
            "45       Setosa          0\n",
            "46       Setosa          0\n",
            "47       Setosa          0\n",
            "48       Setosa          0\n",
            "49       Setosa          0\n",
            "90   Versicolor          2\n",
            "91   Versicolor          2\n",
            "92   Versicolor          2\n",
            "93   Versicolor          2\n",
            "94   Versicolor          2\n",
            "95   Versicolor          2\n",
            "96   Versicolor          2\n",
            "97   Versicolor          2\n",
            "98   Versicolor          2\n",
            "99   Versicolor          2\n",
            "140   Virginica          1\n",
            "141   Virginica          1\n",
            "142   Virginica          1\n",
            "143   Virginica          1\n",
            "144   Virginica          1\n",
            "145   Virginica          1\n",
            "146   Virginica          2\n",
            "147   Virginica          1\n",
            "148   Virginica          1\n",
            "149   Virginica          1\n",
            "Flower: Setosa\n",
            "Mean: [1.4725 0.2575]\n",
            "Variance: [0.03383974 0.01173718]\n",
            "Flower: Versicolor\n",
            "Mean: [4.2025 1.305 ]\n",
            "Variance: [0.21101923 0.03433333]\n",
            "Flower: Virginica\n",
            "Mean: [5.77 2.04]\n",
            "Variance: [0.36011111 0.08488889]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-0381a7160a3e>:68: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  flower_data = train_df[df['variety'] == flower_name]\n",
            "<ipython-input-11-0381a7160a3e>:68: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  flower_data = train_df[df['variety'] == flower_name]\n",
            "<ipython-input-11-0381a7160a3e>:68: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  flower_data = train_df[df['variety'] == flower_name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for feature_name in df.columns:\n",
        "        if feature_name != 'variety':\n",
        "            max_value = df[feature_name].max()\n",
        "            min_value = df[feature_name].min()\n",
        "            result[feature_name] = sigmoid((df[feature_name] - min_value) / (max_value - min_value))\n",
        "    return result\n",
        "\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# 꽃의 종류별로 학습 데이터와 검증 데이터를 나눈다.\n",
        "learnData = pd.concat([df[df['variety'] == 'Setosa'].iloc[:40], \n",
        "                       df[df['variety'] == 'Versicolor'].iloc[:40],\n",
        "                       df[df['variety'] == 'Virginica'].iloc[:40]])  # train\n",
        "evalData = pd.concat([df[df['variety'] == 'Setosa'].iloc[40:], \n",
        "                      df[df['variety'] == 'Versicolor'].iloc[40:],\n",
        "                      df[df['variety'] == 'Virginica'].iloc[40:]])  # test\n",
        "\n",
        "# 학습 데이터와 검증 데이터를 정규화한다.\n",
        "learnDataNorm = normalize(learnData[['petal.width', 'petal.length', 'variety']])\n",
        "evalDataNorm = normalize(evalData[['petal.width', 'petal.length', 'variety']])\n",
        "leranData = learnData[['petal.width', 'petal.length', 'variety']]\n",
        "evalData = evalData[['petal.width', 'petal.length', 'variety']]\n",
        "\n",
        "# 각 꽃의 평균과 분산 계산\n",
        "# print(f'꽃의 평균과 분산 결과')\n",
        "# for lower_name2 in ['Setosa', 'Versicolor', 'Virginica']:\n",
        "#     flower_data = leranData[leranData['variety'] == flower_name][['petal.width', 'petal.length']]\n",
        "#     flower_mean = flower_data.mean()\n",
        "#     flower_var = flower_data.var()\n",
        "#     print(f'Flower: {flower_name}')\n",
        "#     print(f'Mean: {flower_mean.values}')\n",
        "#     print(f'Variance: {flower_var.values}')\n",
        "print(\"-------------------------------------\")\n",
        "print(f'꽃의 평균과 분산  정규화 결과')\n",
        "for flower_name in ['Setosa', 'Versicolor', 'Virginica']:\n",
        "    flower_data = learnDataNorm[learnDataNorm['variety'] == flower_name][['petal.width', 'petal.length']]\n",
        "    flower_mean = flower_data.mean()\n",
        "    flower_var = flower_data.var()\n",
        "    print(f'Flower: {flower_name}')\n",
        "    print(f'Mean: {flower_mean.values}')\n",
        "    print(f'Variance: {flower_var.values}')\n",
        "    \n",
        "#leranData를 랜덤하게 섞어서 randData에 저장장\n",
        "randData = learnData.sample(frac=1).reset_index(drop=True)\n",
        "print(randData, len(randData))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5GQR9fE5f0s",
        "outputId": "285c5bde-a15b-4fec-a623-0bb41db5330a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "꽃의 평균과 분산  정규화 결과\n",
            "Flower: Setosa\n",
            "Mean: [0.51405189 0.51947773]\n",
            "Variance: [1.02860119e-04 5.30432361e-05]\n",
            "Flower: Versicolor\n",
            "Mean: [0.6271284  0.63680425]\n",
            "Variance: [0.00040993 0.00031482]\n",
            "Flower: Virginica\n",
            "Mean: [0.68679106 0.68549154]\n",
            "Variance: [0.00059524 0.00045412]\n",
            "     sepal.length  sepal.width  petal.length  petal.width     variety\n",
            "0             6.8          3.0           5.5          2.1   Virginica\n",
            "1             5.7          2.8           4.5          1.3  Versicolor\n",
            "2             6.7          2.5           5.8          1.8   Virginica\n",
            "3             7.2          3.0           5.8          1.6   Virginica\n",
            "4             5.8          2.7           3.9          1.2  Versicolor\n",
            "..            ...          ...           ...          ...         ...\n",
            "115           6.3          3.3           4.7          1.6  Versicolor\n",
            "116           7.2          3.2           6.0          1.8   Virginica\n",
            "117           6.1          2.8           4.7          1.2  Versicolor\n",
            "118           5.4          3.4           1.5          0.4      Setosa\n",
            "119           4.9          3.6           1.4          0.1      Setosa\n",
            "\n",
            "[120 rows x 5 columns] 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# iris.csv 파일 불러오기\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# petal length와 petal width만 사용\n",
        "#, 2,3 -> petal len , petal wid\n",
        "# sepal len wid , petal len wid\n",
        "#                  2         3\n",
        "data = df.iloc[:, [0,1,2,3]]\n",
        "print(data)\n",
        "# 클래스별 데이터 나누기\n",
        "setosa = data[df['variety'] == 'Setosa']\n",
        "versicolor = data[df['variety'] == 'Versicolor']\n",
        "virginica = data[df['variety'] == 'Virginica']\n",
        "\n",
        "# 평균과 분산 구하기\n",
        "setosa_mean = setosa.mean()\n",
        "versicolor_mean = versicolor.mean()\n",
        "virginica_mean = virginica.mean()\n",
        "\n",
        "setosa_var = setosa.var()\n",
        "versicolor_var = versicolor.var()\n",
        "virginica_var = virginica.var()\n",
        "\n",
        "print('setosa 평균:', setosa_mean)\n",
        "print('versicolor 평균:', versicolor_mean)\n",
        "print('virginica 평균:', virginica_mean)\n",
        "\n",
        "print('setosa 분산:', setosa_var)\n",
        "print('versicolor 분산:', versicolor_var)\n",
        "print('virginica 분산:', virginica_var)\n"
      ],
      "metadata": {
        "id": "B827NwxdAOua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8985d6d2-b463-4f72-a426-67a9942385ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal.length  sepal.width  petal.length  petal.width\n",
            "0             5.1          3.5           1.4          0.2\n",
            "1             4.9          3.0           1.4          0.2\n",
            "2             4.7          3.2           1.3          0.2\n",
            "3             4.6          3.1           1.5          0.2\n",
            "4             5.0          3.6           1.4          0.2\n",
            "..            ...          ...           ...          ...\n",
            "145           6.7          3.0           5.2          2.3\n",
            "146           6.3          2.5           5.0          1.9\n",
            "147           6.5          3.0           5.2          2.0\n",
            "148           6.2          3.4           5.4          2.3\n",
            "149           5.9          3.0           5.1          1.8\n",
            "\n",
            "[150 rows x 4 columns]\n",
            "setosa 평균: sepal.length    5.006\n",
            "sepal.width     3.428\n",
            "petal.length    1.462\n",
            "petal.width     0.246\n",
            "dtype: float64\n",
            "versicolor 평균: sepal.length    5.936\n",
            "sepal.width     2.770\n",
            "petal.length    4.260\n",
            "petal.width     1.326\n",
            "dtype: float64\n",
            "virginica 평균: sepal.length    6.588\n",
            "sepal.width     2.974\n",
            "petal.length    5.552\n",
            "petal.width     2.026\n",
            "dtype: float64\n",
            "setosa 분산: sepal.length    0.124249\n",
            "sepal.width     0.143690\n",
            "petal.length    0.030159\n",
            "petal.width     0.011106\n",
            "dtype: float64\n",
            "versicolor 분산: sepal.length    0.266433\n",
            "sepal.width     0.098469\n",
            "petal.length    0.220816\n",
            "petal.width     0.039106\n",
            "dtype: float64\n",
            "virginica 분산: sepal.length    0.404343\n",
            "sepal.width     0.104004\n",
            "petal.length    0.304588\n",
            "petal.width     0.075433\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron\n",
        "# w(i+1) = w(i) + lrate * Delta * Input\n",
        "# Delta = Err * Gradient\n",
        "# Err = target - output\n",
        "# Gradient = output * (1 - output)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def TLU(x) :\n",
        "    if x >= 0.0 :\n",
        "      return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def sigmoid(x) :\n",
        "    return (1.0 / (1.0 + np.exp(-x)))\n",
        "\n",
        "# x[4][3] t[4]\n",
        "x  = np.array([ \n",
        "      [1.0, 0.0, 0.0], \n",
        "      [1.0, 0.0, 1.0],  \n",
        "      [1.0, 1.0, 0.0],\n",
        "      [1.0, 1.0, 1.0]])\n",
        "t  = np.array([0.0, 0.0, 0.0, 1.0])\n",
        "\n",
        "ev_x  = np.array([ \n",
        "      [1.0, 0.1, 0.1], \n",
        "      [1.0, 0.1, 0.3],  \n",
        "      [1.0, 0.1, 0.7],\n",
        "      [1.0, 0.3, 0.1], \n",
        "      [1.0, 0.7, 0.1],  \n",
        "      [1.0, 0.9, 0.1],      \n",
        "      [1.0, 0.9, 0.9]])\n",
        "\n",
        "\n",
        "\n",
        "lrate = 0.1\n",
        "\n",
        "# W[3]\n",
        "w = np.zeros(3)\n",
        "\n",
        "w[0] = 0.5\n",
        "w[1] = 0.2\n",
        "w[2] = 0.7\n",
        "\n",
        "for epoch in range (500):\n",
        "    print(\"epoch\", epoch)\n",
        "    for i in range(4):\n",
        "        out = w[0]*x[i][0] + w[1]*x[i][1] + w[2]*x[i][2]\n",
        "        y = sigmoid(out)  #TLU(out)\n",
        "        print(t[i], y, out)\n",
        "\n",
        "        for j in range(3) :\n",
        "            w[j] = w[j] + lrate * (t[i] - y) * x[i][j]\n",
        "\n",
        "\n",
        "#########################################\n",
        "print(\"Evaluation!!!\")\n",
        "for i in range(7):\n",
        "  out = w[0]*ev_x[i][0] + w[1]*ev_x[i][1] + w[2]*ev_x[i][2]\n",
        "  y = sigmoid(out)  #TLU(out)\n",
        "  print(y, ev_x[i][1], ev_x[i][2]  )"
      ],
      "metadata": {
        "id": "lq73gxE-Z6Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K-mean \n",
        "import numpy as np;import pandas as pd;import random\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "attri_values = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
        "meana,meanb,meanc = df.iloc[0:51, :], df.iloc[52:101, :], df.iloc[102:151, :]\n",
        "m = random.sample(list(attri_values),3)\n",
        "m1=[];m2=[];m3=[];a=[]\n",
        "def calc_distance(row, means):\n",
        "  sepal_l, sepal_w, petal_l, petal_w = row\n",
        "  mean_sepal_l, mean_sepal_w, mean_petal_l, mean_petal_w = means\n",
        "  distance = (sepal_l - mean_sepal_l) ** 2 + (sepal_w - mean_sepal_w) ** 2 + (petal_l - mean_petal_l) ** 2 + (petal_w - mean_petal_w) ** 2\n",
        "  return distance\n",
        "\n",
        "for i in range(10):\n",
        "    m1 = [];m2 = [];m3 = []\n",
        "    for j in attri_values:\n",
        "        j = list(j)\n",
        "        a = []\n",
        "        distance1 = calc_distance(j, m[0])\n",
        "        distance2 = calc_distance(j, m[1])\n",
        "        distance3 = calc_distance(j, m[2])\n",
        "        a.append(distance1)\n",
        "        a.append(distance2)\n",
        "        a.append(distance3)\n",
        "        if a.index(min(a)) == 0:\n",
        "            m1.append(j)\n",
        "        elif a.index(min(a)) == 1:\n",
        "            m2.append(j)\n",
        "        elif a.index(min(a)) == 2:\n",
        "            m3.append(j)\n",
        "\n",
        "    m1avg = np.mean(np.array(m1), axis=0).tolist() if m1 else m[0]\n",
        "    m2avg = np.mean(np.array(m2), axis=0).tolist() if m2 else m[1]\n",
        "    m3avg = np.mean(np.array(m3), axis=0).tolist() if m3 else m[2]\n",
        "    \n",
        "    m = []\n",
        "    m.append(list(map(lambda x: round(x, 3), m1avg)))\n",
        "    m.append(list(map(lambda x: round(x, 3), m2avg)))\n",
        "    m.append(list(map(lambda x: round(x, 3), m3avg)))\n",
        "\n",
        "print('A의 평균 : ',means_a,'\\n b의 평균 :' ,means_b,'\\n C의 평균 : ',means_c)\n",
        "print('-------------------')\n",
        "for result in m:\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "QYBUhsB3v6GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9c2a205d-0c81-4269-aa9a-67092e3cc12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-06a691d93ca3>:6: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
            "<ipython-input-24-06a691d93ca3>:6: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
            "<ipython-input-24-06a691d93ca3>:6: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  means_a, means_b, means_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-06a691d93ca3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd;from sklearn.neighbors import KNeighborsClassifier\n",
        "count=0\n",
        "data = pd.read_csv(\"iris2.csv\")\n",
        "\n",
        "train_data = pd.concat([data.iloc[0:30], data.iloc[50:80], data.iloc[100:130]])\n",
        "test_data = pd.concat([data.iloc[30:50], data.iloc[80:100], data.iloc[130:151]])\n",
        "\n",
        "train_features = train_data[['SepalLength', 'SepalWidth']]\n",
        "train_labels = train_data['Species']\n",
        "test_features = test_data[['SepalLength', 'SepalWidth']]\n",
        "test_labels = test_data['Species']\n",
        "\n",
        "# KNN 알고리즘 모델 생성\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# 모델 학습\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "# 테스트 데이터 분류 결과 출력\n",
        "for i in range(len(test_features)):\n",
        "    predicted_label = knn.predict([test_features.iloc[i]])[0]\n",
        "    true_label = test_labels.iloc[i]\n",
        "    if predicted_label == true_label:\n",
        "        result = \"맞음\"\n",
        "    else:\n",
        "        result = \"틀림\"\n",
        "        count+=1\n",
        "    print(f\"{i+1}번째 테스트 데이터: {test_features.iloc[i]}, 예측 {predicted_label}, 실제 {true_label} ({result})\")\n",
        "print(\"분류 결과 : \", count, \"틀림\")\n"
      ],
      "metadata": {
        "id": "cp23LM-PHAlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNeighborsClassifier는 사이킷런(Scikit-learn) 라이브러리에서 제공하는 K-최근접 이웃(K-Nearest Neighbor, KNN) 분류 알고리즘의 구현체입니다.\n",
        "\n",
        "KNeighborsClassifier 클래스는 가장 가까운 이웃의 수를 나타내는 n_neighbors 매개변수를 설정하여 모델을 생성합니다.\n",
        "\n",
        "knn 객체를 생성한 후, fit 메서드를 사용하여 모델을 학습시킵니다. fit 메서드는 학습 데이터셋의 특징(feature)과 레이블(label)을 입력으로 받아 모델을 학습합니다. KNN 알고리즘에서는 학습 데이터셋이 모델의 전부이기 때문에, 학습 데이터셋만을 사용하여 모델을 구축합니다.\n",
        "\n",
        "KNeighborsClassifier 클래스의 주요 매개변수는 다음과 같습니다.\n",
        "\n",
        "n_neighbors: 가장 가까운 이웃의 수를 나타냅니다. 일반적으로는 홀수로 지정합니다.\n",
        "weights: 이웃 간 거리에 대한 가중치를 설정합니다. 기본값은 uniform으로 모든 이웃에 동일한 가중치를 부여합니다. distance로 설정하면 이웃 간 거리에 반비례하는 가중치를 부여할 수 있습니다.\n",
        "algorithm: 이웃을 찾는 알고리즘을 설정합니다. ball_tree, kd_tree, brute, auto 중 하나를 선택할 수 있습니다. 기본값은 auto로 자동으로 알고리즘을 선택합니다.\n",
        "metric: 거리 측정 방법을 설정합니다. 기본값은 minkowski이며, p 매개변수를 설정하여 Manhattan distance나 Euclidean distance 등의 거리 측정 방법을 선택할 수 있습니다.\n",
        "KNeighborsClassifier 클래스는 학습된 모델을 바탕으로 예측을 수행할 수 있는 predict 메서드도 제공합니다. predict 메서드는 테스트 데이터셋의 특징을 입력으로 받아 예측 결과를 반환합니다."
      ],
      "metadata": {
        "id": "r-b1UwoLTt-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN알고리즘 - 1-NN  KNeighborsClassifier모듈 사용X\n",
        "\n",
        "import pandas as pd;import numpy as np\n",
        "count = 0;data = pd.read_csv(\"iris.csv\")\n",
        "train_data = pd.concat([data.iloc[0:30], data.iloc[50:80], data.iloc[100:130]])\n",
        "test_data = pd.concat([data.iloc[30:50], data.iloc[80:100], data.iloc[130:150]])\n",
        "\n",
        "train_features = train_data[['sepal.length', 'sepal.width']]\n",
        "train_labels = train_data['variety']\n",
        "test_features = test_data[['sepal.length', 'sepal.width']]\n",
        "test_labels = test_data['variety']\n",
        "\n",
        "def distance(x1, x2):#거리계산\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "# 테스트 데이터 분류 함수 정의\n",
        "def classify(train_features, train_labels, test_features):\n",
        "    min_distance = float(\"inf\") #무한으로 임시정의\n",
        "    for i in range(len(train_features)):\n",
        "        dist = distance(train_features.iloc[i], test_features) #학습데이터, 테스트데이터 거리계산\n",
        "        if dist < min_distance: #최솟값 갱신신\n",
        "            min_distance = dist\n",
        "            predicted_label = train_labels.iloc[i]\n",
        "    return predicted_label \n",
        "\n",
        "for i in range(len(test_features)): #출력\n",
        "    predicted_label = classify(train_features, train_labels, test_features.iloc[i])\n",
        "    true_label = test_labels.iloc[i]\n",
        "    if predicted_label == true_label:\n",
        "        result = \"맞음\"\n",
        "    else:\n",
        "        result = \"틀림\"\n",
        "        count += 1\n",
        "    print(f\"{i+1}번째 테스트 데이터: {test_features.iloc[i]}, 예측 {predicted_label}, 실제 {true_label} ({result})\")\n",
        "print(\"분류 결과 : \", count, \"틀림\")\n",
        "result = count/60*100\n",
        "print(\"정확도 :\",result)\n"
      ],
      "metadata": {
        "id": "1t39RznEReZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN알고리즘 - 3-NN  KNeighborsClassifier모듈 사용X\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "data = pd.read_csv(\"iris.csv\")\n",
        "\n",
        "train_data = pd.concat([data.iloc[0:30], data.iloc[50:80], data.iloc[100:130]])\n",
        "test_data = pd.concat([data.iloc[30:50], data.iloc[80:100], data.iloc[130:150]])\n",
        "\n",
        "train_features = train_data[['sepal.length', 'sepal.width']]\n",
        "train_labels = train_data['variety']\n",
        "test_features = test_data[['sepal.length', 'sepal.width']]\n",
        "test_labels = test_data['variety']\n",
        "\n",
        "# 거리 계산 함수\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "# 모델 예측 함수\n",
        "def predict(train_features, train_labels, test_feature, k):\n",
        "    distances = []\n",
        "    for i in range(len(train_features)):\n",
        "        distance = euclidean_distance(train_features.iloc[i], test_feature)\n",
        "        distances.append((distance, train_labels.iloc[i]))\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    neighbors = distances[:k]\n",
        "    classes = [neighbor[1] for neighbor in neighbors]\n",
        "    prediction = max(set(classes), key=classes.count)\n",
        "    return prediction\n",
        "# 테스트 데이터 분류 결과 출력\n",
        "for i in range(len(test_features)):\n",
        "    predicted_label = predict(train_features, train_labels, test_features.iloc[i], k=3)\n",
        "    true_label = test_labels.iloc[i]\n",
        "    if predicted_label == true_label:\n",
        "        result = \"맞음\"\n",
        "    else:\n",
        "        result = \"틀림\"\n",
        "        count+=1\n",
        "    print(f\"{i+1}번째 테스트 데이터: {test_features.iloc[i]}, 예측 {predicted_label}, 실제 {true_label} ({result})\")\n",
        "print(\"분류 결과 : \", count, \"틀림\")\n",
        "result = count/60*100\n",
        "print(\"정확도 :\",result)\n"
      ],
      "metadata": {
        "id": "ILhqZ-U1Sms3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np;import pandas as pd;import matplotlib as plt;import random;dick={};count=0\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "attri_values = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "flower_name = ['Setosa','Versicolor','Virginica']\n",
        "data_mean=[df.loc[df.variety==flower,['sepal.length','sepal.width','petal.length','petal.width']].mean().values for flower in flower_name]\n",
        "\n",
        "for i,j in enumerate(['A(0~50)','B(52~101)','C(102~151)']):\n",
        "  print(f'{j} 그룹의 평균 열 값 :\\t {data_mean[i]}')\n",
        "for k in data_mean:\n",
        "  distance=[]\n",
        "  for l in  attri_values:\n",
        "    calc_distance = (k[0]-l[0])**2 + (k[1]-l[1])**2 + (k[2]-l[2])**2 + (k[3]-l[3])**2\n",
        "    distance.append(calc_distance)\n",
        "  dick[flower_name[count]]=distance;count+=1\n",
        "print(dick)\n",
        "# csv파일 저장하기\n",
        "result_df=pd.DataFrame(dick)\n",
        "%matplotlib inline\n",
        "result_df.plot()\n",
        "result_df=result_df.transpose()\n",
        "result_df.to_csv('mean_iris.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "K3YXa2I_DXv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "\n",
        "attri_values = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "\n",
        "def calc_distance(row, means):\n",
        "  sepal_l, sepal_w, petal_l, petal_w = row\n",
        "  mean_sepal_l, mean_sepal_w, mean_petal_l, mean_petal_w = means\n",
        "  distance = (sepal_l - mean_sepal_l) ** 2 + (sepal_w - mean_sepal_w) ** 2 + (petal_l - mean_petal_l) ** 2 + (petal_w - mean_petal_w) ** 2\n",
        "  return distance\n",
        "\n",
        "# 기준으로 A1에 Setosa, A2에 Versicolor, A3에 Virginica 저장\n",
        "setosa_mean = df.iloc[0:50, :].mean()\n",
        "versicolor_mean = df.iloc[50:100, :].mean()\n",
        "virginica_mean = df.iloc[100:150, :].mean()\n",
        "\n",
        "# 각 행에 대해 distance_a, distance_b, distance_c 계산하고 결과 저장\n",
        "df_result = pd.DataFrame(columns=['distance_a', 'distance_b', 'distance_c'])\n",
        "\n",
        "for i, row in enumerate(attri_values):\n",
        "    distance_a = calc_distance(row, setosa_mean)\n",
        "    distance_b = calc_distance(row, versicolor_mean)\n",
        "    distance_c = calc_distance(row, virginica_mean)\n",
        "    df_result.loc[i] = [distance_a, distance_b, distance_c]\n",
        "\n",
        "%matplotlib inline\n",
        "df_result.plot()\n",
        "df_result=df_result.transpose()\n",
        "df_result.to_csv('/content/result321.csv', index=False)"
      ],
      "metadata": {
        "id": "2uSS3Ws37pIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "9c1bb24d-a4b4-425f-9195-94d378ca46c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a4a7bf561517>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mattri_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sepal.length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sepal.width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'petal.length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'petal.width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/iris.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#과제4 50개씩 a, b, c로 나눠 각각 요소의 평균을 구한뒤 행마다 유클리드 거리 결과 출력\n",
        "import numpy as np;import pandas as pd\n",
        "df=pd.read_csv('/content/iris.csv')\n",
        "print_attri = df[['sepal.length','sepal.width','petal.length','petal.width']].values\n",
        "mean_a, mean_b, mean_c = df.iloc[0:51, :].mean(), df.iloc[52:101, :].mean(), df.iloc[102:151, :].mean()\n",
        "a_sepal_l,a_sepal_w,a_petal_l,a_petal_w=mean_a['sepal.length'],mean_a['sepal.width'],mean_a['petal.length'],mean_a['petal.width']\n",
        "b_sepal_l,b_sepal_w,b_petal_l,b_petal_w=mean_b['sepal.length'],mean_b['sepal.width'],mean_b['petal.length'],mean_b['petal.width']\n",
        "c_sepal_l,c_sepal_w,c_petal_l,c_petal_w=mean_c['sepal.length'],mean_c['sepal.width'],mean_c['petal.length'],mean_c['petal.width']\n",
        "gap=1\n",
        "print(\"A그룹 : 0행(2)~51행까지의 평균 - \\n\",mean_a,\"\\n----------------------------------------------------------------\")\n",
        "print(\"B그룹 : 52행~101행까지의 평균 - \\n\",mean_b,\"\\n----------------------------------------------------------------\")\n",
        "print(\"C그룹 : 102행~151행까지의 평균 - \\n\",mean_c,\"\\n----------------------------------------------------------------\")\n",
        "for i in range(150):\n",
        "  gap+=1\n",
        "  if i<50:\n",
        "    distance=(a_sepal_l-print_attri[i][0])**2+(a_sepal_w-print_attri[i][1])**2+(a_petal_l-print_attri[i][2])**2+(a_petal_w-print_attri[i][3])**2\n",
        "    print(gap,\"행 distance_a : {:.2f}\".format(distance))\n",
        "    # print(\"distance_a[{}]: {:.2f}\".format(i, distance))\n",
        "  elif i<100:\n",
        "    distance=(b_sepal_l-print_attri[i][0])**2+(b_sepal_w-print_attri[i][1])**2+(b_petal_l-print_attri[i][2])**2+(b_petal_w-print_attri[i][3])**2\n",
        "    print(gap,\"행 distance_b : {:.2f}\".format(distance))\n",
        "  else:\n",
        "    distance=(c_sepal_l-print_attri[i][0])**2+(c_sepal_w-print_attri[i][1])**2+(c_petal_l-print_attri[i][2])**2+(c_petal_w-print_attri[i][3])**2\n",
        "    print(gap,\"행 distance_c : {:.2f}\".format(distance))\n",
        "\n",
        "print( (a_sepal_l-print_attri[1][0])**2 + (a_sepal_w-print_attri[1][1])**2+ (a_petal_l-print_attri[1][2])**2 + (a_petal_w-print_attri[0][3])**2 )"
      ],
      "metadata": {
        "id": "J5biSWi5YXjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "fL=[line for line in csv.reader(open(\"/content/iris2.csv\",\"r\"))][1:]\n",
        "tL=list(map(lambda x:list(map(lambda y:round(sum(map(lambda z:float(fL[x*50+z][1+y]),range(50)))/50,2),range(4))),range(3)))\n",
        "rL=list(map(lambda x:list(map(lambda y:round(sum(map(lambda i:(float(x[1+i])-tL[y][i])**2,range(4))),2),range(3))),fL))\n",
        "print(\"\\n\".join(map(lambda x:f\"{chr(x[0]+65)} 집단 평균 : \"+\", \".join(map(lambda y:str(y),x[1])),enumerate(tL))))\n",
        "print(f\"\\n{'No.':<8}{'A distance':<17}{'B distance':<17}{'C distance':<17}\")\n",
        "print(\"\\n\".join(map(lambda x:f\"{x[0]+1:<8}\"+''.join(map(lambda y:f\"{y:<17}\",x[1])),enumerate(rL))))"
      ],
      "metadata": {
        "id": "Fbata1a6UcuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.인공지능과 기계학습 기본 개념**\n",
        "> 범주 : AI< ML < DL - ANN\n",
        "\n",
        "> DL, ANN 학습예정.\n",
        "지도학습, 비지도학습, 분류, 군집화, 복잡도, overfitting \n",
        "\n",
        "데이터 처리 및 학습 -> 원하는 결과 도출\n",
        "(명시적으로 주어지느냐 아니냐 차이)\n",
        "\n",
        "other is AI\n",
        "\n",
        "\n",
        "1.   패턴인식 알고리즘\n",
        "2.   Expert system(전문가가 가진 지식과 경험을 모델링하여 문제를 해결하는 인공지능)\n",
        "\n",
        "\n",
        "Symbolism vs connectionism\n",
        "\n",
        "응용분야 self driving car\n",
        "\n",
        "---\n",
        "\n",
        "**2. 패턴인식과 기계학습습**\n",
        "- 패턴인식: \n",
        "input data(패턴) 기준에따라 몇개의 패턴의 그룹(클래스)\n",
        "로 나누고, 각 데이터가 어떤 그룹에 해당하는지를 결정.\n",
        "\n",
        "\n",
        "**3. 선형회귀분석**\n",
        "- 단순회귀분석\n",
        "- 최소제곱법(LMS, Least Means Square) : 잔차의 제곱합이 최소가 되도록하는 직선식/ \n",
        "기울기와 절편을 통해 회귀선을 결정되는데 -> 방법 사용\n",
        "\n",
        "> y^-> 예측값 (x1이 들어갔을때 값) / x -> 측정값 (ex: 20명의 몸무게 들(싱수)) \n",
        "잔차를 제곱하는 이유는 +와 - error를 상쇄하기 위해 제곱\n",
        "제곱을 하지 절댓값을 안쓰는 이유? -> 절댓값을 사용할경우 0 이 되는순가 꺾이고 기울기가 무한에 수렴하기떄문에 제곱을 채택.\n",
        "\n",
        "ppt 잔차제곱합 수식 \n",
        "해당수식에서 변하는값은 베타값, x는 상수이며 변하지않음.\n",
        "\n",
        "+ 편미분 제곱빼야됨\n",
        "---------\n",
        "패턴인식 (템플릿 일치)\n",
        "<이론>\n",
        "> 원형데이터와 입력데이터를 비교, 픽셀당 일치하는 색의 개수(겹치는 데이터)가 가장많은 원형데이터 매칭 \n",
        "(이떄 거리계산식 사용)\n",
        "템플릿 전제조건 - 원형데이터가 규칙에 어긋나지 않음. -> 현실적으로 힘듬. \n",
        "\n",
        "- ppt 2_2 패턴인식의 어려움\n",
        "조명변화 / 표정변형/ 기타변형 등의 이유로 \n",
        "패턴인식은 어려움.(초창기)\n",
        "따라서 정교한 방법이 필요함.\n",
        "그결과 기계학습의 등장\n",
        "\n",
        "\n",
        "- 기계학습\n",
        "인간이 갖고있는 고유의 지능적 기능인 학습능력을 기계를 통해 구현하는 방법\n",
        "\n",
        "주어진 데이터들을 분석하여 그로부터 일반적인 규칙이나 새로운 지식을 자동적으로추출해 내는 방법론 개발 \n",
        "\n",
        "----\n",
        "**<중요도 높음>\n",
        "패턴인식 처리과정**\n",
        "\n",
        "> 학습단계 - 인식단계 ppt2_2 7p\n",
        "\n",
        "학습 데이터 셋 + 테스트 데이터 -> 전처리(processing) -> 특징 추출(extraction)\n",
        "1)-> 학습(데이터분석)\n",
        "1-2) -> 결정경계(a와 b를 결정, 나누는 경계)\n",
        "2)-> 분류/인식\n",
        "\n",
        "패턴인식 개발 단계\n",
        "\n",
        "데이터수집-> 전처리 -> 분석 -> **<특징추출기 및 분류기개발>** -> 성능평가-> 수정/보안 -> 전처리로 다시(원하는결과 나올때까지 반복)\n",
        "\n",
        "\n",
        "3. 패턴인식의 기본요소 (ppt9)\n",
        "데이터의 분포 특성\n",
        "2차원 데이터 집합의 산점도,\n",
        "데이터 분포\n",
        "표본추출을 잘해야함. (모집단, 표본집합 차이점 명확히 구별)\n",
        "\n",
        "특징 추출(Feature Extraction)\n",
        " 입력데이터를 그대로 사용하는 대신 각 패턴의특성을 잘 표현해줄 수 있는 핵심정보만 추출하여 사용. -> 대푯값\n",
        "\n",
        "기대효과 : 비용(계산, 메모리)의 증가 및 잡음 등으로 인한 문제해결의 어려움을 감소. \n",
        "\n",
        "방법 -> 원영상 / 격자특징/ 수직히스토그램/ 방향특징\n",
        "ppt14 \n",
        "\n",
        "-------------\n",
        "5)\n",
        "분류 데이터집합x 가 주어졌을때 각 데이터x에 해당하는 클래스 라벨y를 결정\n",
        "\n",
        "결정경계 -> 클래스 구분하는 직선, 곡선,면 \n",
        "g(x1,x2) = x1-x2 = 0 \n",
        "\n",
        "----------\n",
        "6) 분류율 , 오차 ppt17\n",
        " - 분류율 : 분류 \"성공\" 데이터/전체 데이터 *100\n",
        "\n",
        " - 오차 : 분류 \"실패\" 데이터 / 전체데이터 * 100\n",
        "\n",
        "  전체 데이터 = 학습데이터+ 테스트데이터\n",
        "\n",
        "    - 학습오차   사용된 데이터(전체데이터)가 학습데이터를 사용한경우\n",
        "\n",
        "    - 테스트오차 : 사용된 데이터(전체데이터)가 테스트 테이터셋을 사용한 경우 \n",
        "\n",
        "ex)\n",
        "전체데이터 = 1000\n",
        "학습용 데이터 = 600\n",
        "테스트 데이터 = 400\n",
        "Ntrain = 600 = Xtrain = 학습용(트레인)Data \n",
        "Ntest = 400 = Xtest : 평가용 Data\n",
        "\n",
        "ppt 식\n",
        "t(x) - y(x) // t(x)답(이미알고있음) , y(x)=인공지능이 낸 답(학습률에 따라 결과가다름)\n",
        "-> 결과가 0에 수렴해야함. \n",
        "\n",
        "크로네커 델타함수(Kronecker Delt fun) \n",
        "https://ko.wikipedia.org/wiki/%ED%81%AC%EB%A1%9C%EB%84%A4%EC%BB%A4_%EB%8D%B8%ED%83%80\n",
        "\n",
        "이 함수는 두 개의 변수가 같은 값을 가지면 1이 되고, 그렇지 않으면 0이 된다. \n",
        "\n",
        "즉, 전체(데이터셋)중 맞춘(1 인 데이터)개수\n",
        "\n",
        "\n",
        "    - 일반화오차 : 확률(진) 이론적 성능분석을 위해서는 중요, 실제응용에서는 계산 불가. \n",
        "\n",
        "    - 5-분절 교차검증법 처리과정 ppt19\n",
        "    전체데이터 500개 중 학습용 100개, 검증용 400개 \n",
        "    100개씩 5개 셋으로 각각 나온 값을 평균을 내 교차검증 오차를확인 : 5-분절 교차검증법.\n",
        "\n",
        "\n",
        "--------\n",
        "ch4. 패턴인식 관련개념\n",
        "    \n",
        "    - 분류(classfication): 주어진 데이터 집합을 \"이미 정의된\" 클래스로 구분. \n",
        "    - 군집화(clustering) : 입력 데이터의 분포 특성(input 값의 유사성)을 분석해 임의의 복수 개의 그룹으로나눔.\n",
        "\n",
        "2) 교사학습, 비교사학습습\n",
        "\n",
        "    -교사 학습(supervised learning) : 학습시 인식기가 내야할 원하는 출력값을 미리 알려주는 교사가 존재.\n",
        "\n",
        "    -비교사 학습(un): 학습시 인식기의 원하는 출력값에 대한 정보 없이 학습이 이루어지는 형태태\n",
        "--------------\n",
        "**CH4. 군집화**\n",
        "\n",
        "<분류와 군집화는 반드시 구분해야할것>\n",
        "\n",
        "1. 분류, 군집화 \n",
        "    - 분류 : 각 데이터에 대한 클래스 라벨(목표출력값) 정보가 주어짐 -> 교사학습\n",
        "\n",
        "    - 군집화 : 각 데이터에 대한 클래스 라벨 정보가 제공되지 않음 -> 비교사 학습\n",
        "\n",
        "      - inter class가 높으면 클래스간 거리가 멀다.\n",
        "\n",
        "      - intra calss가 낮으면 클래스 내 거리가 가깝다. \n",
        "\n",
        "\n",
        "2. K평균 알고리즘\n",
        "\n",
        "    -주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다.\n",
        "\n",
        "    -1. 시작 : 데이터집합{x1,x2,x3....xn}으로 부터 임의로 k개의 벡터 선택, k개의 초기 대표벡터 집합 {m1,m2,m3...mk}을 생성. \n",
        "    -2 그룹핑 : k 각 데이터 오브젝트들은 가장 가까이 있는 평균값을 기준으로 묶음.\n",
        "\n",
        "    -3 k개의 클러스터의 중심정을 기준으로 평균값이 재조정됨.\n",
        "\n",
        "    -4 수렴할때까지 -2 -3 과정을 반복.\n",
        "\n",
        "3. 계층적 군집화\n",
        "\n",
        "--------------------------\n",
        "**<강조>**\n",
        "분류 (classification)\n",
        " -> 지도학습 = 교사학습 = 목표값 O -> 부류내 거리 (ex : 동전, 지폐 얼마짜리인지 각각 분류 -> 이미 답을 알고있음. )\n",
        "\n",
        "군집 (clusting) -> K-means \n",
        " -> 비지도학습 =비교사 학습 목표값X ->\n",
        " 군집 내 거리 / 군집 간 거리 \n",
        "\n",
        "\n",
        " 군집내 거리가 작고, 군집간 거리가 클수록 좋은 데이터군집.\n",
        "\n",
        " ------------------\n",
        "신경세포 모델링과 신경망의 태동\n",
        "- 생물학적 신경세포\n",
        "수상돌기, 축색돌기, 세포체로 구성.\n",
        "  + **시냅스** -뉴런이라는 신경세포의 부분 중 자극을 세포 밖으로 전도시키는 돌기인 축삭의 끝부분과 신경전달물질이 오가는 다음 뉴런 사이의 틈\n",
        "\n",
        "- 맥클로크 피츠모델 :and or not 게이트구현\n",
        "y = g(시그마 W x) -> ppt5 페이지 그림\n",
        "결과값 = 계단함수 or 시그모이드 함수\n",
        "\n",
        "\n",
        "\n",
        "- net_sum : \n",
        "net sum >= 0 -> 1\n",
        "        <  0 -> 0\n",
        "\n",
        "헤브의 학습규칙\n",
        "- 학습규칙 수식화\n",
        "**ppt 수식 중요**\n",
        "w[i][j] = i번째 세포와 j번째 세포가 연결된 값 이전 w[i][j]번째 값 + 알파ai*bj\n",
        "\n",
        "-> 기본원칙은 헤브의 학습규칙으로 부터 시작.\n",
        "알파 = 학습률 ( 0< 알파 <=1)\n",
        "ai = 신경세포 i의 활성값\n",
        "bj = 신경세포 j의 활성값값\n",
        "\n",
        "---\n",
        "델타규칙\n",
        "용어 설명, 수식\n",
        "목표값보다 활성값이 크면 가중치를 줄임\n",
        "목표값보다 활성값이 작으면 가중치를 올리는 수식 (input에 비례해서 가중치를 조절하는 수식)\n",
        "\n",
        "t = ai(입력값)이 들어왔을때 원하는 목푯값\n",
        "bj = 함수의 출력값\n",
        "ej = tj - bj\n",
        "e값으로 가중치조절\n",
        "\n",
        "방식\n",
        "1) 입력층에 입력패턴을 제시함.\n",
        "2) 신경망을  동작시킴 (y값 출력)\n",
        "3) 타겟값과 y값을 이용해서 델타규칙에 의해 연결 가중치를 조절함.\n",
        "4) 신경망이 완전하게 학습될떄가지 1~3 과정을 반복함.\n",
        "\n",
        "ex) iris.csv 예시\n",
        "sepal.len, sepal.wid, petal,len, petal.wid\n",
        "각각 p1~p4 (4차원데이터)\n",
        "\n",
        "바이어스는 항상 1 \n",
        "\n",
        "--------------------\n",
        "\n",
        "**least mean square , 퍼셉트론**\n",
        "LMS -> 전체 학습데이터에 대한 전체 오차를\n",
        "최소화 하는 방향으로 연결강도를 갱신.\n",
        "입력 x에 대한 목표출력이 y인경우 자승 오차.\n",
        "\n",
        "E = 1/2Err^2 = 1/2(y-hw(x))^2\n",
        "최적의 w를 찾기위해 gradient discent 알고리즘 사용.\n",
        "-> w(n+1) = w(n)-u(aE/aW)\n",
        "\n",
        "연결강도 갱신규칙 ->\n",
        "j -> 가중치가 여러개기때문에 지정해주는것.\n",
        "W[j]New -> W[j]old + 알파 x Err x g'(in) * x[j]\n",
        "\n",
        "------------------------------\n",
        "다층신경망 - 역전파알고리즘\n",
        "XOR연산 불가능 ->암흑기 -> 다층신경망(MLP)역전파알고리즘 등장\n",
        "\n",
        "ebp -> error back prop "
      ],
      "metadata": {
        "id": "uQlvEaYg3KjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd;import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "def TLU(x): #계단식 함수\n",
        "  if x >= 0:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0\n",
        "lrate = 0.1 #학습률 0<lrate<=1 -> leraning late\n",
        "#x[4][3] => bias +1  t[4] bias +1 \n",
        "x = np.array([[1.0,0.0,0.0],\n",
        "              [1.0,0.0,1.0],\n",
        "              [1.0,1.0,0.0],\n",
        "              [1.0,1.0,1.0]])\n",
        "\n",
        "eval_x = np.array([\n",
        "              [1.0,0.1,0.1],\n",
        "              [1.0,0.1,0.9],\n",
        "              [1.0,0.9,0.1],\n",
        "              [1.0,0,9,0.9]]) #맨앞 1은 bias(편향)\n",
        "t = np.array([0.0,0.0,0.0,1.0]) #target \n",
        "w = np.zeros(3)\n",
        "for i in range(3):\n",
        "  w[i] = np.round(np.random.rand(1),2)\n",
        "  # w[i] = np.random.randint(0,1)\n",
        "# w[0] = 0.5\n",
        "# w[1] = 0.2\n",
        "# w[2] = 0.7 #랜덤값으로\n",
        "print('original w[] :',w[0],w[1],w[2])\n",
        "for epoch in range(1000): #에폭시 시행횟수\n",
        "  print('epoch : ',epoch)\n",
        "  for i in range(4): #학습데이터의 양 만큼\n",
        "    out = w[0]*x[i][0] + w[1]*x[i][1] + w[2]*x[i][2]\n",
        "    y = sigmoid(out)\n",
        "    print(t[i], y, out) #target , 활성화함수 값 , 활성화 전 값\n",
        "    for j in range(3):\n",
        "      w[j] = w[j] + lrate * (t[i]-y) * x[i][j]\n",
        "#====================================\n",
        "print(\"====================================\")\n",
        "print(\"evaluation :\");print(w)\n",
        "for i in range(4): #학습데이터의 양 만큼\n",
        "  out = w[0]*eval_x[i][0] + w[1]*eval_x[i][1] + w[2]*eval_x[i][2]\n",
        "  y = sigmoid(out)\n",
        "  print(y, eval_x[i][1],eval_x[i][2]) #target , 활성화함수 값 , 활성화 전 값\n"
      ],
      "metadata": {
        "id": "efQL_F-bQKTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LMS 학습-실습 (XOR 사용시 학습이 제대로 되지않는 코드 예시)\n",
        "import pandas as pd;import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "def TLU(x): #계단식 함수\n",
        "  if x >= 0:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0\n",
        "lrate = 0.1 #학습률 0<lrate<=1 -> leraning late\n",
        "#x[4][3] => bias +1  t[4] bias +1 \n",
        "x = np.array([[1.0,0.0,0.0],\n",
        "              [1.0,0.0,1.0],\n",
        "              [1.0,1.0,0.0],\n",
        "              [1.0,1.0,1.0]])\n",
        "\n",
        "\n",
        "t = np.array([0.0 ,1.0 ,1.0 ,0.0]) #target \n",
        "\n",
        "\n",
        "w = np.zeros(3)\n",
        "for i in range(3):\n",
        "  w[i] = np.round(np.random.rand(1),2)\n",
        "print(f'original w[] :{w[0]},{w[1]},{w[2]}')\n",
        "\n",
        "for epoch in range(2000): #에폭시 시행횟수\n",
        "  print('epoch : ',epoch)\n",
        "  for i in range(4): #학습데이터의 양 만큼 패턴 수만큼.\n",
        "    out = w[0]*x[i][0] + w[1]*x[i][1] + w[2]*x[i][2]\n",
        "    y = sigmoid(out)\n",
        "    print(t[i], y, out) #target , 활성화함수 값 , 활성화 전 값\n",
        "    err = t[i]-y\n",
        "    for j in range(3):\n",
        "      w[j] = w[j] + lrate * err * (y * (1-y)) * x[i][j] #연결강도 갱신규칙\n",
        "#====================================\n",
        "# print(\"====================================\")\n",
        "# print(\"evaluation :\");print(w)\n",
        "# for i in range(4): #학습데이터의 양 만큼\n",
        "#   out = w[0]*eval_x[i][0] + w[1]*eval_x[i][1] + w[2]*eval_x[i][2]\n",
        "#   y = sigmoid(out)\n",
        "#   print(y, eval_x[i][1],eval_x[i][2]) #target , 활성화함수 값 , 활성화 전 값\n"
      ],
      "metadata": {
        "id": "BBH8F5rCgsL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LMS 타겟이 2개일때의 코드\n",
        "#LMS 학습-실습\n",
        "import pandas as pd;import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "\n",
        "lrate = 0.1 #학습률 0<lrate<=1 -> leraning late\n",
        "#x[4][3] => bias +1  t[4] bias +1 \n",
        "x = np.array([[1.0, 0.0,0.0],\n",
        "              [1.0, 0.0,1.0],\n",
        "              [1.0, 1.0,0.0],\n",
        "              [1.0, 1.0,1.0]] )#맨앞 1은 bias(편향)\n",
        "\n",
        "# eval_x = np.array([\n",
        "#               [1.0,0.1,0.1],\n",
        "#               [1.0,0.1,0.9],\n",
        "#               [1.0,0.9,0.1],\n",
        "#               [1.0,0,9,0.9]]) #맨앞 1은 bias(편향)\n",
        "\n",
        "t = np.array([[1.0, 0.0],\n",
        "              [1.0, 0.0],\n",
        "              [1.0, 0.0],\n",
        "              [0.0, 1.0]]) #target \n",
        "y = np.zeros(2)\n",
        "err = np.zeros(2)\n",
        "w = np.zeros([2,3])\n",
        "print(w)\n",
        "for i in range(3):\n",
        "  for j in range(2):\n",
        "    w[j][i] = np.round(np.random.rand(1),2)\n",
        "print(f'original w[] :{w}')\n",
        "\n",
        "for epoch in range(10): #에폭시 시행횟수\n",
        "  print('epoch : ',epoch)\n",
        "  for p in range(4): #학습데이터의 양 만큼 패턴 수만큼.\n",
        "    for j in range(2):\n",
        "      out = w[j][0]*x[p][0] + w[j][1]*x[p][1] + w[j][2]*x[p][2]\n",
        "      y[p] = sigmoid(out)\n",
        "      print(t[p], y[p], out) #target , 활성화함수 값 , 활성화 전 값\n",
        "      err[p] = t[p][j] - y[p] \n",
        "      for j in range(2):\n",
        "        for i in range(3):\n",
        "          w[j][i] = w[j][i] + lrate * err * (y * (1-y[j])) * x[j][i] #연결강도 갱신규칙\n",
        "# ====================================\n",
        "print(\"====================================\");print(\"evaluation :\")\n",
        "# for i in range(4): #학습데이터의 양 만큼\n",
        "#   out = w[0]*eval_x[i][0] + w[1]*eval_x[i][1] + w[2]*eval_x[i][2]\n",
        "#   y = sigmoid(out)\n",
        "#   print(y, eval_x[i][1],eval_x[i][2]) #target , 활성화함수 값 , 활성화 전 값"
      ],
      "metadata": {
        "id": "VR-egbhrkh4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP 알고리즘- xor 예시 코드 \n",
        "import pandas as pd; import numpy as np\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "\n",
        "lrate = 0.1 #학습률 0<lrate<=1 -> leraning late\n",
        "INDIM = 3 #input layer \n",
        "OUTDIM = 2 #output layer\n",
        "H1DIM = 3 #hidden layer\n",
        "PTTN_NUM    = 4\n",
        "x  = np.array([[1.0, 0.0, 0.0 ],  #T-1\n",
        "               [1.0, 0.0, 1.0 ],  #T-2\n",
        "               [1.0, 1.0, 0.0 ],  #T-3\n",
        "               [1.0, 1.0, 1.0 ],  #T-4\n",
        "             ] )\n",
        "                              \n",
        "t  = np.array([ [1.0, 0.0],\n",
        "                [0.0, 1.0],\n",
        "                [0.0, 1.0],\n",
        "                [1.0, 0.0]\n",
        "                ])\n",
        "# w1 = np.zeros([H1DMIN,INDIM])\n",
        "# w2 = np.zeros([H1DMIN,OUTDIM])\n",
        "# for i in range(HIDMIN):\n",
        "#   for j in range(INDIM):\n",
        "#     w1[j][i] = np.round(np.random.rand()/10.0\n",
        "# print(f'original w1[] :{w1}')\n",
        "# for i in range(HIDMIN):\n",
        "#   for j in range(OUTDIM):\n",
        "#     w2[j][i] = np.round(np.random.rand()/10.0\n",
        "# print(f'original w2[] :{w2}')\n",
        "w1 = np.zeros([H1DIM, INDIM])\n",
        "for i in range(H1DIM):\n",
        "    for j in range(INDIM):\n",
        "        w1[i][j] = np.random.rand() / 10.0\n",
        "w2 = np.zeros([OUTDIM, H1DIM])\n",
        "for i in range(OUTDIM):\n",
        "    for j in range(H1DIM):\n",
        "        w2[i][j] = np.random.rand() / 10.0\n",
        "print(f'original w1[] :{w1}')\n",
        "print(f'original w2[] :{w2}')\n",
        "y1 = np.zeros(H1DIM)\n",
        "y2 = np.zeros(OUTDIM)\n",
        "\n",
        "d1 = np.zeros(H1DIM)\n",
        "d2 = np.zeros(OUTDIM)\n",
        "\n",
        "for epoch in range (100000):\n",
        "    if (epoch % 1000) == 0:\n",
        "        print(\"epoch\", epoch)\n",
        "    for p in range(PTTN_NUM):\n",
        "        ###########################################\n",
        "        # feed forwarding\n",
        "        ###########################################\n",
        "        # LAYER-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "            out = 0.0\n",
        "            for j in range(INDIM):\n",
        "                out += w1[i][j] * x[p][j]\n",
        "            y1[i] = sigmoid(out)\n",
        "        # LAYER-2 (Output Layer)\n",
        "        # y1[0] = 1.0  ### bias\n",
        "        \n",
        "        for i in range(OUTDIM):\n",
        "          out = 0.0\n",
        "          for j in range(H1DIM):\n",
        "            out += w2[i][j] * y1[j]\n",
        "          y2[i] = sigmoid(out)\n",
        "\n",
        "        ###########################################\n",
        "        # Back Propagation\n",
        "        ###########################################\n",
        "        # delta(error)  for Layer-2 (Output Layer)\n",
        "        for i in range(OUTDIM):\n",
        "          d2[i] = (t[p][i] - y2[i]) * (y2[i]*(1 - y2[i]))\n",
        "        # delta(error)  for Layer-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "          for j in range(OUTDIM) :\n",
        "            d1[i] = d2[j] * w2[j][i] * (y1[i]*(1 - y1[i]))\n",
        "\n",
        "        ###########################################\n",
        "        # Weight Adjustment\n",
        "        ###########################################\n",
        "        # for Layer-2 (Output Layer)\n",
        "        for i in range(OUTDIM):        \n",
        "          for j in range(H1DIM):\n",
        "            w2[i][j] += lrate * d2[i] * y1[j]\n",
        "\n",
        "        # for Layer-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "            for j in range(INDIM):\n",
        "                w1[i][j] += lrate * d1[i] * x[p][j]\n",
        "\n",
        "        if (epoch % 1000) == 0:\n",
        "            print(\"pattern number[%d] : \" % p)\n",
        "            print(\"target : \", \"%7.2f\" % t[p][0],  \"%7.2f\" % t[p][1])\n",
        "            print(\"output : \", \"%7.2f\" % y2[0],  \"%7.2f\" % y2[1])\n",
        "\n",
        "#########################################################\n",
        "# Evaluation\n",
        "\n",
        "PLOT_SIZE = 21\n",
        "px = np.zeros(PLOT_SIZE)\n",
        "py = np.zeros(PLOT_SIZE)\n",
        "for k in range(PLOT_SIZE):\n",
        "    # feed forwarding\n",
        "    # LAYER-1 (Hidden Layer)\n",
        "    x[0][1] = k/(PLOT_SIZE-1)\n",
        "    for i in range(H1DIM):\n",
        "        out = 0.0\n",
        "        for j in range(INDIM):\n",
        "            out += w1[i][j] * x[0][j]\n",
        "            y1[i] = sigmoid(out)\n",
        "    # LAYER-2 (Output Layer)\n",
        "    out = 0.0\n",
        "    for i in range(H1DIM):\n",
        "        out += w2[i] * y1[i]\n",
        "    y2 = sigmoid(out)\n",
        "\n",
        "    print(\"%7.2f\" % x[0][1], \"%7.2f\" % y2)\n",
        "    px[k] = x[0][1]\n",
        "    py[k] = y2\n"
      ],
      "metadata": {
        "id": "rKKWyI4fjojm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP\n",
        "# 4 classes : T/C/E/L\n",
        "# 3 patterns / 1 class\n",
        "# 2 Layer\n",
        "import pandas as pd; import numpy as np\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "\n",
        "lrate = 0.1 #학습률 0<lrate<=1 -> leraning late\n",
        "INDIM = 26 #input layer \n",
        "OUTDIM = 4 #output layer\n",
        "H1DIM = 10 #hidden layer\n",
        "PTTN_NUM    = 12\n",
        "x  = np.array([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-1\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,\n",
        "                     0.0, 0.0, 1.0, 0.0, 1.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-2\n",
        "               [1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0, \n",
        "                     0.0, 0.0, 1.0, 0.0, 0.0 ],  #T-3\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-1\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #C-2\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #C-3\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-1\n",
        "               [1.0, 0.5, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #E-2\n",
        "               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     0.5, 1.0, 1.0, 1.0, 1.0 ],  #E-3               \n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 1.0 ],  #L-1\n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 1.0, \n",
        "                     1.0, 1.0, 1.0, 1.0, 0.0 ],  #L-2\n",
        "               [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     1.0, 0.0, 0.0, 0.0, 0.0, \n",
        "                     0.3, 1.0, 1.0, 1.0, 1.0 ] ] )  #L-3                            \n",
        "                              \n",
        "t  = np.array([ [1.0, 0.0, 0.0, 0.0],\n",
        "                [1.0, 0.0, 0.0, 0.0],\n",
        "                [1.0, 0.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 1.0, 0.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 1.0, 0.0],\n",
        "                [0.0, 0.0, 0.0, 1.0],\n",
        "                [0.0, 0.0, 0.0, 1.0],\n",
        "                [0.0, 0.0, 0.0, 1.0] ])\n",
        "w1 = np.zeros([H1DIM, INDIM])\n",
        "for i in range(H1DIM):\n",
        "    for j in range(INDIM):\n",
        "        w1[i][j] = np.random.rand() / 10.0\n",
        "\n",
        "w2 = np.zeros([OUTDIM, H1DIM])\n",
        "for i in range(OUTDIM):\n",
        "    for j in range(H1DIM):\n",
        "        w2[i][j] = np.random.rand() / 10.0\n",
        "\n",
        "print(w1)\n",
        "print(w2)\n",
        "\n",
        "y1 = np.zeros(H1DIM)\n",
        "y2 = np.zeros(OUTDIM)\n",
        "\n",
        "d1 = np.zeros(H1DIM)\n",
        "d2 = np.zeros(OUTDIM)\n",
        "\n",
        "for epoch in range (20000):\n",
        "    if (epoch % 1000) == 0:\n",
        "        print(\"epoch\", epoch)\n",
        "    for p in range(PTTN_NUM):\n",
        "        ###########################################\n",
        "        # feed forwarding\n",
        "        ###########################################\n",
        "        # LAYER-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "            out = 0.0\n",
        "            for j in range(INDIM):\n",
        "                out += w1[i][j] * x[p][j]\n",
        "            y1[i] = sigmoid(out)\n",
        "        # LAYER-2 (Output Layer)\n",
        "        # y1[0] = 1.0  ### bias\n",
        "        \n",
        "        for i in range(OUTDIM):\n",
        "          out = 0.0\n",
        "          for j in range(H1DIM):\n",
        "            out += w2[i][j] * y1[j]\n",
        "          y2[i] = sigmoid(out)\n",
        "\n",
        "        ###########################################\n",
        "        # Back Propagation\n",
        "        ###########################################\n",
        "        # delta(error)  for Layer-2 (Output Layer)\n",
        "        for i in range(OUTDIM):\n",
        "          d2[i] = (t[p][i] - y2[i]) * (y2[i]*(1 - y2[i]))\n",
        "        # delta(error)  for Layer-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "          for j in range(OUTDIM) :\n",
        "            d1[i] = d2[j] * w2[j][i] * (y1[i]*(1 - y1[i]))\n",
        "\n",
        "        ###########################################\n",
        "        # Weight Adjustment\n",
        "        ###########################################\n",
        "        # for Layer-2 (Output Layer)\n",
        "        for i in range(OUTDIM):        \n",
        "          for j in range(H1DIM):\n",
        "            w2[i][j] += lrate * d2[i] * y1[j]\n",
        "\n",
        "        # for Layer-1 (Hidden Layer)\n",
        "        for i in range(H1DIM):\n",
        "            for j in range(INDIM):\n",
        "                w1[i][j] += lrate * d1[i] * x[p][j]\n",
        "\n",
        "        if (epoch % 1000) == 0:\n",
        "            print(\"pattern number[%d] : \" % p)\n",
        "            print(\"target : \", \"%7.2f\" % t[p][0],  \"%7.2f\" % t[p][1], \"%7.2f\" % t[p][2], \"%7.2f\" % t[p][3])\n",
        "            print(\"output : \", \"%7.2f\" % y2[0],  \"%7.2f\" % y2[1], \"%7.2f\" % y2[2], \"%7.2f\" % y2[3])\n",
        "print(\"test\")\n",
        "for i in range(H1DIM):\n",
        "    out = 0.0\n",
        "    for j in range(INDIM):\n",
        "        out += w1[i][j]* z[j]\n",
        "    z1[i] = sigmoid(out)\n",
        "for i in range(OUTDIM):\n",
        "    out = 0.0\n",
        "    for j in range(H1DIM):\n",
        "        out += w2[i][j] * z1[j]\n",
        "    z2[i] = sigmoid(out)\n",
        "    \n",
        "print(\"%7.2f\"%z2[0], \"%7.2f\"%z2[1], \"%7.2f\"%z2[2], \"%7.2f\"%z2[3], )"
      ],
      "metadata": {
        "id": "EecRJwecnEke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron\n",
        "# w(i+1) = w(i) + lrate * Delta * Input\n",
        "# Delta = Err * Gradient\n",
        "# Err = target - output\n",
        "# Gradient = output * (1 - output)\n",
        "# LMS 타겟이 2개일때의 코드\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def sigmoid(x) :\n",
        "    return (1.0 / (1.0 + np.exp(-x)))\n",
        "\n",
        "x  = np.array([ \n",
        "    [1.0, 0.0, 0.0], \n",
        "    [1.0, 0.0, 1.0],  \n",
        "    [1.0, 1.0, 0.0],\n",
        "    [1.0, 1.0, 1.0]])\n",
        "t  = np.array([ \n",
        "    [1.0, 0.0], \n",
        "    [1.0, 0.0],  \n",
        "    [1.0, 0.0],\n",
        "    [0.0, 1.0]])\n",
        "\n",
        "lrate = 0.1\n",
        "\n",
        "w = np.zeros([2,3])\n",
        "y = np.zeros(2)\n",
        "err = np.zeros(2)\n",
        "\n",
        "for j in range(2) :\n",
        "  for i in range(3) :\n",
        "    w[j][i] = random.random()\n",
        "print(w)\n",
        "\n",
        "for epoch in range (2000):\n",
        "    print(\"epoch\", epoch)\n",
        "    for p in range(4):\n",
        "        for j in range(2):\n",
        "          out = w[j][0]*x[p][0] + w[j][1]*x[p][1] + w[j][2]*x[p][2]\n",
        "          y[j] = sigmoid(out)\n",
        "\n",
        "        print(t[p], y)\n",
        "        for j in range(2):\n",
        "          err[j] = t[p][j] - y[j]\n",
        "\n",
        "        for j in range(2) :\n",
        "          for i in range(3) :\n",
        "            w[j][i] = w[j][i] + lrate * err[j] * (y[j] * (1 - y[j]))* x[p][i]\n"
      ],
      "metadata": {
        "id": "5-UFT2IVQvpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN알고리즘 - 3-NN  KNeighborsClassifier모듈 사용X\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "data = pd.read_csv(\"iris.csv\")\n",
        "\n",
        "train_data = pd.concat([data.iloc[0:30], data.iloc[50:80], data.iloc[100:130]])\n",
        "test_data = pd.concat([data.iloc[30:50], data.iloc[80:100], data.iloc[130:150]])\n",
        "\n",
        "train_features = train_data[['sepal.length', 'sepal.width']]\n",
        "train_labels = train_data['variety']\n",
        "test_features = test_data[['sepal.length', 'sepal.width']]\n",
        "test_labels = test_data['variety']\n",
        "\n",
        "#활성화 함수 TLU(계단함수) , Sigmoid 함수\n",
        "def sigmoid(x):\n",
        "  return (1.0 / (1.0 + np.exp(-x))) # sigmoid fun 수식 : 1 / 1+ e^-x \n",
        "def TLU(x): #계단식 함수\n",
        "  if x >= 0:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "# 거리 계산 함수\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "# 모델 예측 함수\n",
        "def predict(train_features, train_labels, test_feature, k):\n",
        "    distances = []\n",
        "    for i in range(len(train_features)):\n",
        "        distance = euclidean_distance(train_features.iloc[i], test_feature)\n",
        "        distances.append((distance, train_labels.iloc[i]))\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    neighbors = distances[:k]\n",
        "    classes = [neighbor[1] for neighbor in neighbors]\n",
        "    prediction = max(set(classes), key=classes.count)\n",
        "    return prediction\n",
        "# 테스트 데이터 분류 결과 출력\n",
        "for i in range(len(test_features)):\n",
        "    predicted_label = predict(train_features, train_labels, test_features.iloc[i], k=3)\n",
        "    true_label = test_labels.iloc[i]\n",
        "    if predicted_label == true_label:\n",
        "        result = \"맞음\"\n",
        "    else:\n",
        "        result = \"틀림\"\n",
        "        count+=1\n",
        "    print(f\"{i+1}번째 테스트 데이터: {test_features.iloc[i]}, 예측 {predicted_label}, 실제 {true_label} ({result})\")\n",
        "print(\"분류 결과 : \", count, \"틀림\")\n",
        "result = count/60*100\n",
        "print(\"정확도 :\",result)\n"
      ],
      "metadata": {
        "id": "KSRGazGtnLSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-29 과제\n",
        "액셀 저장, matplotlib 표시\n",
        "\n",
        "m1 m2 m3\n",
        "클러스터링 알고리즘 찾아내기\n",
        "\n",
        "step 1. m1, m2, m3 랜덤 배정\n",
        "step 2. xi -> m1, m2, m3중에서 거리 구하기\n",
        " (150번) (라벨링 단계 - 편나누기(Groupping)\n",
        "step 3. 대표 수정하기 (m1 = 1/|Ck| (c1그룹의 평균 / m2 = c2그룹의 평균, m3 = ...)\n",
        "\n",
        "\n",
        "150개 데이터중 random choice 해서나온값\n",
        "\n",
        "KNN알고리즘\n",
        "A그룹 - 30개 학습용 데이터 / 20개 테스트 데이터\n",
        "\n",
        "b ..c\n",
        "\n",
        "1-NN -> 학습데이터30개 평균 -> 대표\n",
        "b, c 모두 .\n",
        "\n",
        "20개중 1번째인거 -> a,b,c인지 ? 학습이 끝난 a b c랑 거리를 구해서 가장 작은값을 해당하는 곳으로 분류함, \n",
        "-----------\n",
        "3nn -> 가까운값 3개 -> 어느쪽에 더 많이 속하는지 투표 -> 해당값으로 분류.\n",
        "\n",
        "04-20\n",
        "input을 x1, x2 -> 0.1단위로 변화\n",
        "x1 (0 0.1 0.2 0.3 ... 1.0)\n",
        "x2 (0 0.1 0.2 0.3 ... 1.0)\n",
        "이중포문 돌리면 약 100개가 생김.\n",
        "출력 -> 그래프 표현\n",
        "\n",
        "\n",
        "-----------\n",
        "시험 퍼셉트론 x\n",
        "지폐 얼굴인식 시스템\n",
        "거리구하기\n",
        "실기 2~3문제\n",
        "\n",
        "퍼셉트론 학습규칙 유도 -> 기말말\n",
        "\n",
        "--------------\n",
        "TCEL -. 패턴3개씩 학습\n",
        "학습끝난후 테스트?\n",
        "학습끝난 결과 이어서 input 받은후 삐딱한 t가 나왔을때 뭐가나오는지\n"
      ],
      "metadata": {
        "id": "rB-3qc59Tr8S"
      }
    }
  ]
}